# Servers, Datacenters & Google Case Study  
---

##  What is a Server?
- A **server** is a computer that provides **services** to other computers (clients)
- Servers are designed for:
  - High **reliability**
  - Handling **large numbers of requests**
- Organizations usually need **many servers** for different services:
  - Web servers
  - Email servers
  - Database servers
- Modern server hardware is:
  - More **powerful**
  - More **compact**

---

##  Compact Servers
Organizations want to **save physical space**, so compact servers are used.

Benefits:
- Less **floor space** required
- Easier **management**
- Better **scalability**
- Lower **power and cooling** requirements

---

##  Racks
- Servers and equipment are placed inside **racks**
- Hardware is built to fit into **rack units**:
  - 1U, 2U, etc.
- A single rack can hold **up to 42 (1U) servers**

---

##  Blade Servers & Blade Enclosures
### Blade Servers
- A **blade server** is a simplified, modular computer
- Designed to be lightweight and efficient

### Blade Enclosure
- Holds multiple blade servers
- Provides:
  - Power
  - Cooling
  - Network interfaces

---

##  What is a Data Center?
A **data center** is a facility that houses:
- Servers
- Networking equipment
- Storage systems
- Cooling systems
- UPS (Uninterruptible Power Supply)
- Air filters

Key points:
- Contains **large numbers of networked computers**
- Can be:
  - One room
  - Multiple floors
  - An entire building

---

##  Scale of Modern Data Centers
- Microsoft alone has **tens of millions of servers**
- Shows how massive cloud infrastructure really is

---

##  Data Center Components
- **Air Conditioning**
  - Keeps equipment within safe temperature limits
- **Redundant Power**
  - UPS
  - Generators
  - Multiple power feeds
- **Connectivity**
  - High-speed network connections

---

##  Power in Data Centers
- Typical efficiency: **1.7**
  - For every 1W used by servers, **0.7W is wasted**
- Major challenge:
  - **Reducing power costs**

---

##  Google’s Top-Secret Data Centers
- This infrastructure is **what makes Google Google**
- Contains:
  - Thousands of miles of fiber
  - Thousands of servers

### What Google’s Data Centers Can Do
- Index **20 billion web pages per day**
- Handle **3+ billion searches daily**
- Run **millions of ad auctions** in real time
- Provide free Gmail storage to **425 million users**
- Stream millions of **YouTube videos daily**

---

##  Google Back in 1999
- Google rented space in an **Exodus data center**
- Servers were placed in a shared facility
- Search results took **3.5 seconds**
- Systems often **crashed on Mondays**

---

##  Mission “Willpower”
Google decided to:
- Build and operate **its own data centers**
- Reduce **setup and operational costs**
- Become the company with the **largest number of servers**

Important note:
- The **exact number** of servers doesn’t matter
- **Efficiency matters more**

---

##  Electricity & Cooling Management
### Traditional Method
- Large air conditioners
- Very high energy consumption

### Google’s Insight
- Servers do NOT need extreme cold

---

##  Smart Cooling Strategy
- Cold aisle temperature: **~80°F**
- Hot aisle temperature: **up to 120°F**
- Hot air absorbed using **water-filled coils**
- Heated water is:
  - Pumped out
  - Cooled
  - Re-circulated

---

##  Water Cooling Innovations
- Uses **cooling towers** instead of chillers
- Hot water flows through giant radiators
- In different locations:
  - **Belgium:** Industrial canal water
  - **Finland:** Seawater

---

##  Even Water is Recycled
- Uses **non-drinking (non-potable) water**
- Cleans water just enough for cooling
- Examples:
  - Douglas County: city wastewater
  - Belgium: industrial canal
- Rainwater is also **captured and reused**

---

##  Power Backup Optimization
- Instead of huge UPS systems:
  - Small backup batteries placed near each server
- Efficiency measured using **PUE (Power Usage Effectiveness)**

### PUE Values
- Ideal: **1.0**
- Good: **2.0**
- Google achieved: **1.2** (exceptional)

---

##  What Else to Optimize?
###  Hardware!

---

##  Google Hardware Strategy
- Google is not just an internet company
- It is one of the **largest hardware manufacturers**
- Builds its **own servers**

### Early Days
- Built 2,000 servers from basic parts
- Cost:
  - Google server: **$1,500**
  - Market server: **$5,000**

---

##  Custom Hardware Design
Google removes unnecessary parts:
- No graphics cards
- No enclosures
- Motherboards placed directly into racks
- Same approach used for networking equipment

---

##  Access Speed Optimization
- Bought abandoned **fiber-optic networks**
- Paid very low prices
- For YouTube:
  - Mini data centers near ISPs
  - Store popular videos close to users

---

##  Software at Google
- Treats thousands of servers as **one giant computer**
- Developers can control machines easily
- Technologies:
  - MapReduce
  - Hadoop
  - Borg

---

##  Reliability Engineering
- Dedicated **Site Reliability Engineering (SRE)** teams
- SREs are engineers, not just troubleshooters
- Conduct **DiRT (Disaster Recovery Testing)** every year

---

##  Failures Still Happen
- April 17, 2012:
  - Gmail went down for **1.4% of users**
- Cause:
  - A change pushed by an engineer on Friday

---

##  How Many Servers Does Google Have?
- **49,923 servers** in the Lenoir facility
- July 9, 2008: **Google’s millionth server**
- Goal:
  - Make existing data centers **obsolete** through innovation

---

##  Utilization in Data Centers
- 10%–30% utilization is considered **good**

### Reasons for Low Utilization
- Uneven resource usage
  - Apps use CPU, memory, disk unevenly
- Long provisioning times
- Uncertain demand
- Risk management (extra servers kept idle)

---

##  Conclusion
- Data centers are massive and complex
- Cloud computing helps **use this infrastructure efficiently**


---

## ❓ Questions?
